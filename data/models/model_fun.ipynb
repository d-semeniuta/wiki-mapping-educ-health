{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKI_PATH = os.path.abspath('../raw/wikipedia/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                          name         category  \\\n",
      "0   0  A\" Fort and Battery Hill Redoubt-Camp Early\"         building   \n",
      "1   1                          Lockkeeper's\" House\"            house   \n",
      "2   2                                       &moshik       restaurant   \n",
      "3   3                                      'A'akapa  populated place   \n",
      "4   4                                     'Abadilah  populated place   \n",
      "\n",
      "         lat         lon  \n",
      "0  38.789444  -77.427778  \n",
      "1  40.885278  -92.196944  \n",
      "2  52.376622    4.905381  \n",
      "3  -8.819444 -140.130556  \n",
      "4  25.438333   56.191389  \n"
     ]
    }
   ],
   "source": [
    "# load article meta data of IDs, names, categories, geolocations\n",
    "article_meta_data = pd.read_csv(WIKI_PATH + '/All_Image_Coordinates_2.csv')\n",
    "print(article_meta_data.iloc[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56721 available .npy files\n"
     ]
    }
   ],
   "source": [
    "# validate .npy files\n",
    "# produce and save list of available, properly linked .npy files assuming IDs match up\n",
    "def validate_npy_files(ids, path_to_npy):\n",
    "    # ids: numpy array of article ID indices\n",
    "    \n",
    "    validated_ids = []\n",
    "    for id in np.nditer(ids):\n",
    "        npy_path_i = os.path.join(path_to_npy, str(id) + '.npy')\n",
    "        if os.path.exists(npy_path_i):\n",
    "            validated_ids.append(id)\n",
    "    \n",
    "    validated_ids = pd.DataFrame(validated_ids, columns=['Article IDs'])\n",
    "    return validated_ids\n",
    "\n",
    "ids = article_meta_data.loc[:, 'id'].to_numpy()\n",
    "validated_ids = validate_npy_files(ids, os.path.join(WIKI_PATH, 'doc2vec_embeddings'))\n",
    "print('{} available .npy files'.format(len(validated_ids)))\n",
    "validated_ids.to_csv(WIKI_PATH + '/available_npy_ids.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model (just copy Sheehan, use softmax with cross-entropy or MSE for education level percentages)\n",
    "class WikiEmbRegressor(nn.Module):\n",
    "    def __init__(emb_size=300, n_embs=10, ave_embs=True, concat=False, MEL_IMR=True):\n",
    "        '''\n",
    "        NN model for regression of maternal education level (MEL) and infant mortality rate (IMR)\n",
    "        emb_size: (int) size of input Wikipedia article embeddings\n",
    "        n_embs: (int) number of articles used in a single forward run\n",
    "        ave_embs: (Bool) whether to average embeddings or to concatenate them (not sure what concat was used for in original code, ave_embs seems to imply NOT concat)\n",
    "        concat: (Boolean) whether to concatenate embeddings or to simply use one of them?\n",
    "        MEL_IMR: (Boolean) whether to predict MEL (True) or IMR (False)\n",
    "        '''\n",
    "        \n",
    "        super(WikiEmbRegressor, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.n_embs = n_embs\n",
    "        self.ave_embs = ave_embs\n",
    "        np.random.seed(1234)\n",
    "        self.concat = concat\n",
    "        self.MEL_IMR = MEL_IMR\n",
    "        \n",
    "        if self.concat:\n",
    "            self.input_shape = self.emb_size * self.n_embs + self.n_embs\n",
    "        else:\n",
    "            self.input_shape = self.emb_size + 1\n",
    "        \n",
    "        if MEL_IMR:\n",
    "            self.model = nn.Sequential(\n",
    "                            nn.Linear(self.input_shape, 512), nn.ReLU(),\n",
    "                            nn.Linear(512, 256), nn.ReLU(),\n",
    "                            nn.Linear(256, 32), nn.ReLU(),\n",
    "                            nn.Linear(32, 4)\n",
    "            )\n",
    "        else:\n",
    "            self.model = nn.Sequential(\n",
    "                            nn.Linear(self.input_shape, 512), nn.ReLU(),\n",
    "                            nn.Linear(512, 256), nn.ReLU(),\n",
    "                            nn.Linear(256, 32), nn.ReLU(),\n",
    "                            nn.Linear(32, 1)\n",
    "            )\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        \n",
    "    def forward(embs, dists):\n",
    "        # embs: Torch tensor, shape (batch_size, n_embs, emb_size)\n",
    "        # dists: Torch tensor, shape (batch_size, n_embs, 1)\n",
    "        \n",
    "        if self.ave_embs:\n",
    "            embs = embs.mean(dim=1)\n",
    "            dists = dists.mean(dim=1)\n",
    "        else:\n",
    "            embs = embs.reshape((embs.shape[0], -1))\n",
    "            dists = dists.reshape((dists.shape[0], -1))\n",
    "            \n",
    "        inputs = torch.concat([embs, dists], dim=-1)\n",
    "        \n",
    "        return self.model(inputs)\n",
    "        \n",
    "#         model = Sequential()\n",
    "#         if concat:\n",
    "#             model.add(Dense(512, input_shape=(input_shape,), kernel_initializer='normal', activation='sigmoid'))\n",
    "#         else:\n",
    "#             model.add(Dense(512, input_shape=(300,), kernel_initializer='normal', activation='sigmoid'))\n",
    "#         model.add(Dense(256, kernel_initializer='normal', activation='sigmoid'))\n",
    "#         model.add(Dense(32, kernel_initializer='normal', activation='sigmoid'))\n",
    "#         model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "#         model.compile(loss='mean_squared_error', optimizer='adam', metrics=[coeff_determination, 'mae'])\n",
    "#         print(model.summary())\n",
    "\n",
    "# get closest N articles in available article set to clusters, take mean embedding with mean distance to start\n",
    "# build train/val/test data set, save data sets in .csv files, but don't reload, check if already loaded\n",
    "# training loop, model saving, checkpointing and validation/performance\n",
    "# GPU integration\n",
    "# determine countries to start with: English speaking\n",
    "\n",
    "# make model diagram\n",
    "# outline slides\n",
    "# redo baselines with article embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:testgdal] *",
   "language": "python",
   "name": "conda-env-testgdal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
